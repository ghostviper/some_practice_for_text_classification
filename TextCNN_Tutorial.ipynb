{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于TextCNN的文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入必要的包\n",
    "本教程采用Python3.5+、Tensorflow1.8+ 进行讲解\n",
    "\n",
    "Tensorflow是Google开源的并行神经网络计算框架，可以利用CPU,GPU,TPU资源对神经网络进行并行计算，底层采用C++实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义建模需要的全局变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本序列的长度\n",
    "sequence_length = 200\n",
    "# 分类个数\n",
    "num_classes = 10\n",
    "\n",
    "# 定义embedding_size\n",
    "embedding_size = 200\n",
    "\n",
    "# 定义词表大小\n",
    "vocab_size = 50000\n",
    "\n",
    "# 不同卷积层卷积核的定义\n",
    "filter_sizes = [2, 3, 4, 5]\n",
    "\n",
    "# 输出特征图数目\n",
    "num_filters = 16\n",
    "\n",
    "# dropout 保留比例\n",
    "dropout_keep_prob = 0.7\n",
    "\n",
    "# 正则化\n",
    "l2_reg_lambda = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义输入输出变量\n",
    "\n",
    "通常输入是指输入到神经网络建模的变量x\n",
    "输出一般是预测的变量或者标签y\n",
    "\n",
    "**placeholder:** placeholder 是 Tensorflow 中的占位符，暂时储存变量，可以定义输入张量的类型，输入张量的形状\n",
    "\n",
    "**variable:**  variable可以是一个初始化的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输入的变量x：表示输入一条长度为sequence_length的句子或文章\n",
    "x = tf.placeholder(dtype=tf.int64, shape=[None, sequence_length], name=\"input_x\")\n",
    "# 定义输出的变量y: 表示分类输出的概率\n",
    "y = tf.placeholder(dtype=tf.int64, shape=[None, num_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding层定义\n",
    "Embedding：简单来说，就是应对高维、稀疏的id类特征，通过将单个id（可能是某个词的id，也可能是某个商品的id）映射成一个稠密向量，变id特征的“精确匹配”为embedding向量的“模糊查找”，从而提升算法的扩展能力。\n",
    "\n",
    "举个例子：如果要表示一组文本序列单词所在的位置 \\[\"我\", \"是\", \"中国人\", \"。\"\\]该怎么表示呢？\n",
    "\n",
    "方法1： \\[3, 2, 4, 1\\],分别表示词语“我”、“是”、“中国人”和句号所处的位置分别为3， 2， 4， 1 \n",
    "\n",
    "方法2：\\[[0, 0, 1, 0], \n",
    "[0, 1, 0, 0], \n",
    "[0, 0, 0, 1], \n",
    "[1, 0, 0, 0]\\] \n",
    "\n",
    "其中两种表示都是表示词语出现的位置，其中方法一是该位置的稠密表示，方法二为稀疏表示\n",
    "\n",
    "tf.embedding_lookup:\n",
    "所谓embedding_lookup(W, id1)，可以想像成一个只在id1位为1的\\[1, vocab_size\\]的one_hot向量，与\\[vocab_size, embed_size\\]的W矩阵相乘，结果是一个\\[1, embed_size\\]的向量，它就是id1对应的embedding向量，实际上就是W矩阵的第id1行。\n",
    "\n",
    "![](./images/lookup.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对输入的文本序列进行embedding_lookup训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义embedding_lookup 所需要的权重矩阵W\n",
    "W = tf.Variable(tf.random_uniform(shape=[vocab_size, embedding_size]), name=\"embedding_w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对输入的文本序列进行embedding_lookup训练\n",
    "embedding_layer = tf.nn.embedding_lookup(W, x)\n",
    "# 注意此时embedding_layer的输出维度为 [batch_size, vocab_size, embedding_size] 如果要进行conv2d卷积必须输入4维的张量\n",
    "embedding_layer = tf.expand_dims(embedding_layer, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积层的定义\n",
    "\n",
    "- **常规卷积层工作原理：**\n",
    "\n",
    "![](./images/conv_net.gif)\n",
    "\n",
    "- **TextCNN 卷积神经网络原理：**\n",
    "\n",
    "![](./images/textcnn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **卷积层需定义的参数:**\n",
    "\n",
    "**卷积核：** Tensorflow API要求输入的卷积核大小为 \\[**高度**, **宽度**, **输入的通道数**, **输出的通道数**\\]\n",
    "\n",
    "**步长定义：** strides定义为 一组int型数值表示卷积核在输入的4维张量在每个维度上滑动的步幅大小\n",
    "\n",
    "**填充方式** padding定义为\"SAME\",\"VALID\"两种方式\n",
    "\n",
    "- **池化层需定义的参数:**\n",
    "\n",
    "**ksize：**  输入张量在每个维度上的窗口大小\n",
    "\n",
    "**步长定义：** strides定义为 一组int型数值表示卷积核在输入的4维张量在每个维度上滑动的步幅大小\n",
    "\n",
    "**填充方式** padding定义为\"SAME\",\"VALID\"两种方式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.max_pool??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layers = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "    w = tf.Variable(tf.truncated_normal(shape=filter_shape, stddev=0.1))\n",
    "    conv = tf.nn.conv2d(\n",
    "        input=embedding_layer,\n",
    "        filter=,\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID'\n",
    "    )\n",
    "    # 疑问？？？ 卷积层输出维度是多少呢？\n",
    "    #\n",
    "    # Bias定义\n",
    "    bias = tf.constant(0.5, [num_filters])\n",
    "    # activation_function(w*x+b)\n",
    "    # 对卷积层进行激活\n",
    "    conv_layer = tf.nn.relu(tf.nn.bias_add(conv, bias))\n",
    "    # 对卷积层输出进行最大值池化\n",
    "    pool_layer = tf.nn.max_pool(\n",
    "        value=conv_layer,\n",
    "        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding=\"VALID\"\n",
    "    )\n",
    "    output_layers.append(pool_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将池化后的卷积层拼接起来展开成一个稠密的向量\n",
    "num_filters_total = num_filters*len(filter_sizes)\n",
    "pool_flatten = tf.reshape(output_layers, [-1, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对展开的稠密向量做dropout\n",
    "dropout_layer = tf.nn.dropout(pool_flatten, keep_prob=dropout_keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输出层的定义\n",
    "\n",
    "输出层可以将卷积后池化层的拼接向量进行线性分类激活\n",
    "\n",
    "即对输出层进行建模：soft_max(W*x + b）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(64)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_layer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.get_variable(\n",
    "    name=\"W\",\n",
    "    shape=[num_filters_total, num_classes],\n",
    "    initializer=tf.contrib.layers.xavier_initializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.Variable(tf.constant(0.1, shape=[num_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = tf.constant(0.0)\n",
    "l2_loss += tf.nn.l2_loss(W)\n",
    "l2_loss += tf.nn.l2_loss(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.nn.xw_plus_b(dropout_layer, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数评估输出层的损失大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(entropy) + l2_reg_lambda * l2_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对损失进行优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器，目标是最小化loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-4).minimize(loss)\n",
    "# 输入预处理的数据进行迭代优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性能评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits, axis=1), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
